{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from torch.optim import AdamW\n",
    "from torch.utils import data\n",
    "from sklearn import metrics\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import torch\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/Users/annapalatkina/Desktop/big_5_PT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:52:56,131 : INFO : Training with seed 42...\n",
      "2023-05-24 22:52:56,132 : INFO : Model name xlm-roberta-base...\n",
      "2023-05-24 22:52:56,132 : INFO : Tokenizer name xlm-roberta-base...\n",
      "2023-05-24 22:52:56,132 : INFO : Prediction column Экстраверсия_5...\n",
      "2023-05-24 22:52:56,133 : INFO : Learning rate 4e-05...\n",
      "2023-05-24 22:52:56,133 : INFO : Dropout  0.3...\n",
      "2023-05-24 22:52:56,133 : INFO : Epochs 1...\n",
      "2023-05-24 22:52:56,134 : INFO : Max length 10...\n",
      "2023-05-24 22:52:56,135 : INFO : Min length 0...\n",
      "2023-05-24 22:52:56,135 : INFO : Batch size 4...\n",
      "2023-05-24 22:52:56,136 : INFO : Warmup steps 1000...\n",
      "2023-05-24 22:52:56,136 : INFO : Freeze False...\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed_value=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    return seed_value\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "_ = seed_everything(42)\n",
    "logger.info(f\"Training with seed {42}...\")\n",
    "model_name = 'xlm-roberta-base'\n",
    "logger.info(f\"Model name {model_name}...\")\n",
    "tokenizer_name = 'xlm-roberta-base'\n",
    "logger.info(f\"Tokenizer name {tokenizer_name}...\")\n",
    "prediction_column = 'Экстраверсия_5'\n",
    "logger.info(f\"Prediction column {prediction_column}...\")\n",
    "LR = 4e-5\n",
    "logger.info(f\"Learning rate {LR}...\")\n",
    "dropout = 0.3\n",
    "logger.info(f\"Dropout  {dropout}...\")\n",
    "epochs = 1\n",
    "logger.info(f\"Epochs {epochs}...\")\n",
    "maxl = 10\n",
    "logger.info(f\"Max length {maxl}...\")\n",
    "minl = 0\n",
    "logger.info(f\"Min length {minl}...\")\n",
    "bsize = 4\n",
    "logger.info(f\"Batch size {bsize}...\")\n",
    "warmup_steps = 1000\n",
    "logger.info(f\"Warmup steps {warmup_steps}...\")\n",
    "freeze = False\n",
    "logger.info(f\"Freeze {freeze}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(labels, texts, cur_tokenizer, cur_device):\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long).to(cur_device)\n",
    "    encoding = cur_tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=maxl,\n",
    "    ).to(cur_device)\n",
    "    return labels_tensor, encoding\n",
    "\n",
    "def create_ids_mask(dataset):\n",
    "    texts = dataset.post_text.to_list()\n",
    "    labels = dataset[prediction_column].to_list()\n",
    "    labels_tensor, encoding = encoder(labels, texts, tokenizer, device)\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "    return data.TensorDataset(input_ids, attention_mask, labels_tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 22:52:56,180 : INFO : Using device: cpu...\n",
      "2023-05-24 22:53:00,862 : INFO : Train dataset with oversampling shape: (214824, 2)...\n",
      "2023-05-24 22:53:00,862 : INFO : Valid dataset shape: (37459, 2)...\n",
      "2023-05-24 22:53:00,863 : INFO : Test dataset shape: (37460, 2)...\n",
      "2023-05-24 22:53:00,865 : INFO : We have 5 classes\n",
      "2023-05-24 22:53:00,865 : INFO : Tokenizing with max length 10...\n",
      "2023-05-24 22:53:24,483 : INFO : Tokenizing finished.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "dataset = pd.read_csv('/Users/annapalatkina/Desktop/big_5_PT/data/df_texts_ocean.csv')\n",
    "dataset['lengths'] = dataset.post_text.apply(lambda x: len(str(x).split()))\n",
    "df = dataset.query('lengths > @minl')[['post_text', prediction_column]]\n",
    "\n",
    "train_data, valid_data = train_test_split(df, test_size=0.3, stratify=df[prediction_column], random_state=42)\n",
    "valid_data, test_data = train_test_split(valid_data, test_size=0.5, stratify=valid_data[prediction_column], random_state=42)\n",
    "\n",
    "# Oversampling\n",
    "# !!!!!!!!!! СДЕЛАТЬ ОВЕРСЭМПЛИНГ ДЛЯ КАЖДОЙ ЧЕРТЫ РАЗНЫЙ !!!!!!!!!!\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "train_data = pd.concat([train_data,\n",
    "                    train_data.query('Экстраверсия_5 == 0'),\n",
    "                    train_data.query('Экстраверсия_5 == 0'),\n",
    "                    train_data.query('Экстраверсия_5 == 4'),\n",
    "                    train_data.query('Экстраверсия_5 == 4'),\n",
    "                    train_data.query('Экстраверсия_5 == 4'),\n",
    "                    train_data.query('Экстраверсия_5 == 4')])\n",
    "    \n",
    "train_data = shuffle(train_data)\n",
    "train_data.reset_index(inplace=True, drop=True)\n",
    "valid_data.reset_index(inplace=True, drop=True)\n",
    "test_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "logger.info(f\"Train dataset with oversampling shape: {train_data.shape}...\") \n",
    "logger.info(f\"Valid dataset shape: {valid_data.shape}...\")\n",
    "logger.info(f\"Test dataset shape: {test_data.shape}...\")\n",
    "\n",
    "num_classes = train_data[prediction_column].nunique()\n",
    "logger.info(f\"We have {num_classes} classes\")\n",
    "\n",
    "logger.info(f\"Tokenizing with max length {maxl}...\")\n",
    "train_dataset = create_ids_mask(train_data)\n",
    "train_iter = data.DataLoader(train_dataset, batch_size=bsize, shuffle=True) \n",
    "\n",
    "dev_dataset = create_ids_mask(valid_data)\n",
    "dev_iter = data.DataLoader(dev_dataset, batch_size=bsize, shuffle=False)\n",
    "\n",
    "test_dataset = create_ids_mask(test_data)\n",
    "test_iter = data.DataLoader(test_dataset, batch_size=bsize, shuffle=False)\n",
    "logger.info(\"Tokenizing finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_ids, attention_mask, labels_tensor in train_iter:\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0708, -0.0036, -0.0123, -0.0418,  0.0447],\n",
       "        [ 0.0810, -0.0207, -0.0213, -0.0324,  0.0488],\n",
       "        [ 0.0722, -0.0138, -0.0271, -0.0307,  0.0479],\n",
       "        [ 0.0763, -0.0066, -0.0184, -0.0268,  0.0353]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import RNN\n",
    "model = RNN(model_name=model_name,num_classes=num_classes,freeze_bert=True, attention=True).to(device)\n",
    "model(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4211,  0.3321, -0.2421,  0.0454, -0.0578],\n",
       "        [ 0.4188,  0.3093, -0.2395,  0.0343, -0.0471],\n",
       "        [ 0.4185,  0.3121, -0.2403,  0.0360, -0.0484],\n",
       "        [ 0.4224,  0.3311, -0.2408,  0.0443, -0.0574]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import CNN\n",
    "model = CNN(model_name=model_name,num_classes=num_classes,freeze_bert=True, attention=True).to(device)\n",
    "model(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1112, -0.0197, -0.2082,  0.0607,  0.1781],\n",
       "        [ 0.1077, -0.0245, -0.2160,  0.0728,  0.1435],\n",
       "        [ 0.1138, -0.0374, -0.2074,  0.0559,  0.1829],\n",
       "        [ 0.0869, -0.0272, -0.2291,  0.0663,  0.1662]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import CNN_LSTM\n",
    "model = CNN_LSTM(model_name=model_name,num_classes=num_classes,freeze_bert=True, attention=True).to(device)\n",
    "model(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2331,  0.3372,  0.0433,  0.1241, -0.1168],\n",
       "        [ 0.2185,  0.3141,  0.0783,  0.1195, -0.1413],\n",
       "        [ 0.2153,  0.3142,  0.0818,  0.1148, -0.1319],\n",
       "        [ 0.2131,  0.3121,  0.0721,  0.1236, -0.1338]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import Bert_sequence_classifcation\n",
    "model = Bert_sequence_classifcation(model_name=model_name,num_classes=num_classes).to(device)\n",
    "model(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBilstmClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, freeze_bert=True):\n",
    "        super(BertBilstmClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 128, num_classes\n",
    "        # Instantiate BERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.linear_relu = nn.Sequential(\n",
    "            nn.Linear(2*H, H), # if BiLSTM\n",
    "            # nn.Linear(H, H), # if LSTM\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(H, H), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(H, H)\n",
    "        )\n",
    "        # LSTM\n",
    "        # self.bilstm = nn.LSTM(D_in, H, batch_first = False, bidirectional=False)\n",
    "        # BiLSTM\n",
    "        self.bilstm = nn.LSTM(D_in, H, batch_first = False, bidirectional=True, dropout=0.2, num_layers=3)\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False        \n",
    "        self.out = nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Feed input to BERT\n",
    "        bert_last_hidden_state = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask).last_hidden_state   # [Batch_size, max_length, 768]\n",
    "        print(f'Bert shape: {bert_last_hidden_state.shape}')   \n",
    "\n",
    "        output_lstm, (h_0, h_1) =  self.bilstm(bert_last_hidden_state)\n",
    "        print(f'Output lstm shape: {output_lstm.shape}')    # [Batch_size, max_length, 2*H]\n",
    "        x = output_lstm \n",
    "        attention_weights = torch.matmul(x, x.transpose(-1, -2))  # Compute attention weights\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)  # Apply softmax to get attention probabilities\n",
    "        x = torch.matmul(attention_weights, x)  # Apply attention\n",
    "        x = x[:, 0]  # Use the [CLS] token representation for aggregation  # [Batch_size, 2*H]\n",
    "   \n",
    "        print(f'CLS representation shape: {x.shape}')\n",
    "\n",
    "        # Linear layers + dropout + relu \n",
    "        x = self.linear_relu(x)    # [Batch_size, H]\n",
    "        print(f'Shape after linear + relu: {x.shape}')\n",
    "\n",
    "        # residual connection\n",
    "        #x = torch.cat([bigru_cnn.squeeze(dim=2), bilstm_cnn.squeeze(dim=2)], dim=1)\n",
    "\n",
    "        # Feed x to classifier to compute logits\n",
    "        logits = self.out(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BertBilstmClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, freeze_bert=True):\n",
    "        super(BertBilstmClassifier, self).__init__()\n",
    "\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out, H_fc = 768, 128, num_classes, 32\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False   \n",
    "\n",
    "        # LSTM\n",
    "        # self.bilstm = nn.LSTM(D_in, H, batch_first = False, bidirectional=False)\n",
    "        # BiLSTM\n",
    "        self.bilstm = nn.LSTM(D_in, H, batch_first = False, bidirectional=True, dropout=0.2, num_layers=3)\n",
    "\n",
    "        # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=768,\n",
    "                      out_channels= [2, 2, 2][i],\n",
    "                      kernel_size=[2, 3, 4][i])\n",
    "            for i in range(len([2, 3, 4]))\n",
    "        ])\n",
    "\n",
    "        # 2 liner layers + activation\n",
    "        self.linear_relu = nn.Sequential(\n",
    "            nn.Linear(12, H_fc), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(H_fc, H_fc), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(H_fc, H_fc)\n",
    "        )\n",
    "        self.fc =  nn.Linear(256, 6)\n",
    "        self.out = nn.Linear(H_fc, D_out)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Feed input to BERT\n",
    "        bert_last_hidden_state = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask).last_hidden_state   # [Batch_size, max_length, 768]\n",
    "\n",
    "        # CNN\n",
    "        bert_reshaped = bert_last_hidden_state.permute(0, 2, 1)\n",
    "        # Apply CNN and ReLU. Output shape: (Batch_size, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(bert_reshaped)) for conv1d in self.conv1d_list]\n",
    "        # Max pooling. Output shape: (Batch_size, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (Batch_size, sum(num_filters))\n",
    "        x_cnn = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "\n",
    "\n",
    "        output_lstm, (h_0, h_1) =  self.bilstm(bert_last_hidden_state) # [Batch_size, max_length, 2*H]\n",
    "        x = output_lstm \n",
    "        attention_weights = torch.matmul(x, x.transpose(-1, -2))  # Compute attention weights\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)  # Apply softmax to get attention probabilities\n",
    "        x = torch.matmul(attention_weights, x)  # Apply attention\n",
    "        x = x[:, 0]  # Use the [CLS] token representation for aggregation  # [Batch_size, 2*H]\n",
    "        x_lstm = self.fc(x)\n",
    "\n",
    "        cnn_lstm = torch.cat((x_cnn,x_lstm),dim=1)\n",
    "        # Linear layers + dropout + relu \n",
    "        x = self.linear_relu(cnn_lstm)   # [Batch_size, H]\n",
    "\n",
    "        # Feed x to classifier to compute logits\n",
    "        logits = self.out(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertBilstmClassifier(model_name=model_name,num_classes=num_classes,freeze_bert=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0183, -0.1331, -0.0812, -0.0150,  0.0642],\n",
       "        [ 0.0185, -0.1434, -0.0862, -0.0258,  0.0461],\n",
       "        [ 0.0318, -0.1345, -0.0934, -0.0381,  0.0760],\n",
       "        [ 0.0076, -0.1390, -0.0819, -0.0365,  0.0887]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3.10",
   "language": "python",
   "name": "venv_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
